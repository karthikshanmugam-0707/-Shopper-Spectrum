
transaction_volume = data1.groupby('Country')['InvoiceNo'].nunique().sort_values(ascending=False).head(10)
fig = px.pie(transaction_volume, values=transaction_volume.values, names=transaction_volume.index, title='Top 10 Countries by Transaction Volume',hole=0.5)
fig.show()

top_products = data1.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)
fig = px.pie(top_products, values=top_products.values, names=top_products.index, title='Top 10 Products by Quantity Sold', hole=0.5)
fig.show()

data1['InvoiceDate'] = pd.to_datetime(data1['InvoiceDate'])

monthly_sales = data1.set_index('InvoiceDate').resample('ME').sum(numeric_only=True)

plt.figure(figsize=(12,6))
sns.lineplot(x=monthly_sales.index, y=monthly_sales['Quantity'], marker='o', color='darkred')
plt.title('Monthly Sales Volume')
plt.xlabel('Month')
plt.ylabel('Quantity Sold')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# monetary distribution of transactions
data1['TotalPrice'] = data1['Quantity'] * data1['UnitPrice']
transaction_totals = data1.groupby('InvoiceNo')['TotalPrice'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(12,6))
sns.barplot(x=transaction_totals.index, y=transaction_totals.values, color='red')
plt.title('Top 10 Transactions by Total Value')
plt.xlabel('Invoice Number')
plt.ylabel('Total Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


#monetary distribution of customers
customer_totals = data1.groupby('CustomerID')['TotalPrice'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(12,6))
sns.barplot(x=customer_totals.index, y=customer_totals.values, color='black')
plt.title('Top 10 Customers by Total Spend')
plt.xlabel('Customer ID')
plt.ylabel('Total Spend')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Ensure InvoiceDate is datetime
data1['InvoiceDate'] = pd.to_datetime(data1['InvoiceDate'])

data1['TotalPrice'] = data1['Quantity'] * data1['UnitPrice']

snapshot_date = data1['InvoiceDate'].max() + pd.Timedelta(days=1)
rfm = data1.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,   # Recency
    'InvoiceNo': 'nunique',                                     # Frequency
    'TotalPrice': 'sum'                                         # Monetary
}).rename(columns={
    'InvoiceDate': 'Recency',
    'InvoiceNo': 'Frequency',
    'TotalPrice': 'Monetary'
})


# Histograms for each RFM component
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

rfm['Recency'].plot(kind='hist', bins=30, ax=axes[0], color='skyblue', edgecolor='black')
axes[0].set_title('Recency Distribution')

rfm['Frequency'].plot(kind='hist', bins=30, ax=axes[1], color='lightgreen', edgecolor='black')
axes[1].set_title('Frequency Distribution')

rfm['Monetary'].plot(kind='hist', bins=30, ax=axes[2], color='salmon', edgecolor='black')
axes[2].set_title('Monetary Distribution')

plt.tight_layout()
plt.show()

sse = []  
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(rfm)
    sse.append(kmeans.inertia_)

plt.figure(figsize=(8,5))
plt.plot(k_range, sse, marker='o')
plt.title('Elbow Curve for Optimal K')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (SSE)')
plt.xticks(k_range)
plt.grid(True)
plt.tight_layout()
plt.show()

# Assign clusters to each customer
rfm['Cluster'] = kmeans.predict(rfm[['Recency', 'Frequency', 'Monetary']])


cluster_summary = rfm.groupby('Cluster').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean',
    
}).rename_axis('Cluster')
cluster_summary['NumCustomers'] = rfm.groupby('Cluster').size()

# Plotting clusters
plt.figure(figsize=(12,6))
sns.scatterplot(data=rfm, x='Recency', y='Monetary', hue='Cluster', palette='Set2')
plt.title('Customer Segments Based on RFM')
plt.show()

product_user_matrix = data1.pivot_table(
    index='CustomerID',
    columns='Description',
    values='Quantity',
    aggfunc='sum',
    fill_value=0
)


product_similarity = cosine_similarity(product_user_matrix.T)

# Convert to DataFrame with product names
product_similarity_df = pd.DataFrame(product_similarity,
                                     index=product_user_matrix.columns,
                                     columns=product_user_matrix.columns)

top_products = data1.groupby('Description')['Quantity'].sum().nlargest(20).index
top_similarity = product_similarity_df.loc[top_products, top_products]

plt.figure(figsize=(12,10))
sns.heatmap(top_similarity, cmap='YlGnBu', linewidths=0.5)
plt.title('Product Similarity Heatmap (Top 20)')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

#clustering methodology

# Calculate Recency, Frequency, and Monetary 
snapshot_date = data1['InvoiceDate'].max() + pd.Timedelta(days=1)

recency_df = data1.groupby('CustomerID')['InvoiceDate'].max().reset_index()
recency_df['Recency'] = (snapshot_date - recency_df['InvoiceDate']).dt.days
recency_df = recency_df[['CustomerID', 'Recency']]

frequency_df = data1.groupby('CustomerID')['InvoiceNo'].nunique().reset_index(name='Frequency')

if 'TotalPrice' not in data1.columns:
	data1['TotalPrice'] = data1['Quantity'] * data1['UnitPrice']

monetary_df = data1.groupby('CustomerID')['TotalPrice'].sum().reset_index(name='Monetary')

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
rfm_standardized = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])
rfm_standardized_df = pd.DataFrame(rfm_standardized, columns=['Recency', 'Frequency', 'Monetary'])

scaler

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=4, random_state=42)
rfm['Cluster'] = kmeans.fit_predict(rfm_standardized)

from sklearn.metrics import silhouette_score

silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(rfm_standardized)
    score = silhouette_score(rfm_standardized, labels)
    silhouette_scores.append(score)

plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis')
plt.show()

# Fit the KMeans model
kmeans = KMeans(n_clusters=4, random_state=42)
rfm['Cluster'] = kmeans.fit_predict(rfm_standardized)
print(rfm['Cluster'].value_counts())

cluster_profiles = rfm.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean().round(2)
print(cluster_profiles)

cluster_labels = {
    '0': 'Regular',
    '1': 'At-Risk',
    '2': 'High-Value',
    '3': 'occational'
}

rfm['Segment'] = rfm['Cluster'].map(cluster_labels)

cluster_labels

import plotly.express as px

fig = px.scatter_3d(
    rfm, x='Recency', y='Frequency', z='Monetary',
    color='Segment', opacity=0.8
)
fig.show()

fig = px.scatter_3d(
    rfm,
    x='Recency',
    y='Frequency',
    z='Monetary',
    color='Cluster',  
    opacity=0.8,
    size_max=12,
    title='3D RFM Customer Segments (Interactive)',
    labels={'Recency', 'Frequency', 'Monetary'}
)

fig.update_traces(marker=dict(size=6))
fig.update_layout(margin=dict(l=0, r=0, b=0, t=40))
fig.show()

import pickle

# Save KMeans model
with open('rfm_kmeans_model.pkl', 'wb') as file:
    pickle.dump(kmeans, file)


# Save the scaler 
with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

from sklearn.metrics.pairwise import cosine_similarity

user_item_matrix = product_user_matrix

item_similarity = cosine_similarity(user_item_matrix.T)
similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)

def get_similar_items(item_id, n=5):
    return similarity_df[item_id].sort_values(ascending=False)[1:n+1]

with open('item_similarity.pkl', 'wb') as file:
    pickle.dump(item_similarity, file)

import pickle

product_lookup = data1[['Description', 'StockCode']].drop_duplicates()

name_to_code = dict(zip(product_lookup['Description'], product_lookup['StockCode']))
code_to_name = dict(zip(product_lookup['StockCode'], product_lookup['Description']))
product_names = [code_to_name[code] if code in code_to_name else code for code in user_item_matrix.columns]

with open('name_to_code.pkl', 'wb') as f:
    pickle.dump(name_to_code, f)

with open('code_to_name.pkl', 'wb') as f:
    pickle.dump(code_to_name, f)

with open('product_list.pkl', 'wb') as f:
    pickle.dump(product_names, f)

